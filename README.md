# DatasetGen - AI-First Dataset Creation Platform

An advanced platform that transforms raw enterprise data into high-quality ML training datasets through intelligent processing, multi-modal fusion, and visual pipeline building. Built with cutting-edge LLMs including Claude Sonnet 4, GPT-4, and Gemini, achieving 30x efficiency improvements through DocETL-inspired optimization.

## ğŸš€ Key Features

### Intelligence Layer
- **Claude Sonnet 4 Integration**: Extended thinking and vision capabilities for document understanding
- **Multi-LLM Support**: OpenAI GPT-4, Google Gemini, and Claude models
- **DocETL-Inspired Optimization**: 30x efficiency improvement in pipeline execution
- **Intelligent Schema Detection**: Automatic type inference and data understanding

### Data Processing
- **Multi-Modal Fusion**: MDF Framework implementation with 5 fusion strategies
- **Visual Pipeline Builder**: Drag-and-drop interface with React Flow
- **Quality Validation**: 10 types of automated checks with severity levels
- **PII Detection**: Built-in patterns for SSN, email, phone, credit cards, IP addresses
- **PDF Processing**: Table extraction using Claude Vision API

### Monitoring & Operations
- **Real-Time Dashboard**: Track pipeline executions, costs, and performance
- **Cost Tracking**: Detailed breakdown by LLM provider, pipeline, and resource type
- **Prometheus Metrics**: Production-ready monitoring integration
- **Distributed Execution**: Celery-based pipeline orchestration

### Enterprise Features
- **Security**: JWT authentication, role-based access control
- **Data Connectors**: CSV, JSON with auto-detection and LLM enhancement
- **Caching**: Redis-based caching for LLM responses
- **API Documentation**: Auto-generated OpenAPI/Swagger docs

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Frontend (Next.js + React Flow)          â”‚
â”‚   â€¢ Visual Pipeline Builder                  â”‚
â”‚   â€¢ Monitoring Dashboard                     â”‚
â”‚   â€¢ Dataset Management                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚        API Layer (FastAPI)                   â”‚
â”‚   â€¢ RESTful Endpoints                        â”‚
â”‚   â€¢ WebSocket Support                        â”‚
â”‚   â€¢ OpenAPI Documentation                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚     Intelligence Layer                       â”‚
â”‚   â€¢ Claude Sonnet 4 (Extended Thinking)      â”‚
â”‚   â€¢ GPT-4 / Gemini Integration              â”‚
â”‚   â€¢ DocETL Pipeline Optimizer               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚     Processing Layer                         â”‚
â”‚   â€¢ Multi-Modal Fusion Engine               â”‚
â”‚   â€¢ Quality Validation System               â”‚
â”‚   â€¢ PII Detection & Masking                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚     Data Layer                              â”‚
â”‚   â€¢ PostgreSQL (Metadata)                   â”‚
â”‚   â€¢ Redis (Caching & Metrics)              â”‚
â”‚   â€¢ S3/Local Storage (Datasets)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- Docker and Docker Compose (recommended)
- Or manually install:
  - Python 3.11+
  - Node.js 18+
  - PostgreSQL 15+
  - Redis 7+

### Setup Instructions

1. **Clone the repository:**
```bash
git clone https://github.com/yourorg/dataset-gen.git
cd dataset-gen
```

2. **Configure environment:**
```bash
cp .env.example .env
```

Edit `.env` and add your API keys:
```env
# Required for Claude Sonnet 4
ANTHROPIC_API_KEY=your_anthropic_api_key

# Optional for additional LLMs
OPENAI_API_KEY=your_openai_api_key
GOOGLE_API_KEY=your_google_api_key

# Generate secret key
SECRET_KEY=$(openssl rand -hex 32)
```

3. **Start with Docker Compose:**
```bash
docker-compose up -d
```

4. **Access the platform:**
- ğŸŒ Frontend: http://localhost:3000
- ğŸ”§ Backend API: http://localhost:8000
- ğŸ“š API Documentation: http://localhost:8000/docs
- ğŸ“Š Monitoring: http://localhost:3000/monitoring

5. **Create an account:**
- Navigate to http://localhost:3000/register
- Create your account
- Login to access the platform

## ğŸ“ Project Structure

```
dataset-gen/
â”œâ”€â”€ backend/                      # FastAPI backend
â”‚   â”œâ”€â”€ api/v1/endpoints/        # REST API endpoints
â”‚   â”œâ”€â”€ core/                    # Core configuration
â”‚   â”œâ”€â”€ models/                  # SQLAlchemy models
â”‚   â”œâ”€â”€ services/               
â”‚   â”‚   â”œâ”€â”€ llm/                # LLM integrations (Claude, GPT, Gemini)
â”‚   â”‚   â”œâ”€â”€ pipeline/           # Pipeline execution & optimization
â”‚   â”‚   â”œâ”€â”€ connectors/         # Data source connectors
â”‚   â”‚   â”œâ”€â”€ multimodal/         # PDF, image processing
â”‚   â”‚   â”œâ”€â”€ quality/            # Validation & PII detection
â”‚   â”‚   â””â”€â”€ monitoring/         # Metrics collection
â”‚   â”œâ”€â”€ schemas/                # Pydantic schemas
â”‚   â””â”€â”€ tests/                  # Test suites
â”œâ”€â”€ frontend/                    # Next.js frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/         # React components
â”‚   â”‚   â”‚   â”œâ”€â”€ PipelineBuilder/  # Visual pipeline builder
â”‚   â”‚   â”‚   â””â”€â”€ charts/           # Monitoring charts
â”‚   â”‚   â”œâ”€â”€ pages/             # Next.js pages
â”‚   â”‚   â”œâ”€â”€ services/          # API clients
â”‚   â”‚   â””â”€â”€ styles/            # CSS modules
â”‚   â””â”€â”€ public/                # Static assets
â”œâ”€â”€ infrastructure/            
â”‚   â”œâ”€â”€ docker/               # Dockerfiles
â”‚   â””â”€â”€ nginx/                # Nginx configuration
â”œâ”€â”€ examples/                 # Example pipelines
â”œâ”€â”€ docs/                     # Documentation
â””â”€â”€ docker-compose.yml        # Container orchestration
```

## ğŸ’» Development

### Manual Setup (without Docker)

**Backend:**
```bash
cd backend
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt

# Setup database
alembic upgrade head

# Start backend
uvicorn main:app --reload --port 8000
```

**Frontend:**
```bash
cd frontend
npm install
npm run dev
```

**Start required services:**
```bash
# PostgreSQL
docker run -d -p 5432:5432 -e POSTGRES_PASSWORD=platform_pass postgres:15

# Redis
docker run -d -p 6379:6379 redis:7-alpine
```

### Running Tests

```bash
# Backend tests
cd backend
pytest -v
pytest --cov=services tests/  # With coverage

# Frontend tests
cd frontend
npm test
```

## ğŸ“Š Usage Examples

### Creating a Pipeline via API

```python
import requests

# Create a pipeline
pipeline = {
    "name": "Customer Data Quality Pipeline",
    "description": "Validate and clean customer data",
    "config": {
        "steps": [
            {
                "type": "quality_validation",
                "config": {
                    "deep_analysis": True,
                    "min_quality_score": 85.0
                }
            },
            {
                "type": "pii_detection",
                "config": {
                    "action": "mask"
                }
            }
        ]
    }
}

response = requests.post(
    "http://localhost:8000/api/v1/pipelines",
    json=pipeline,
    headers={"Authorization": f"Bearer {token}"}
)
```

### Visual Pipeline Builder

1. Navigate to `/pipeline-builder`
2. Drag nodes from the sidebar
3. Connect nodes to create data flow
4. Configure each step's parameters
5. Save and execute the pipeline

## ğŸ”§ Configuration

### Pipeline Optimization Settings

```yaml
optimization:
  enable_caching: true
  cache_ttl: 3600
  enable_parallelization: true
  enable_filter_pushdown: true
  enable_projection_pushdown: true
  batch_size: 1000
```

### LLM Configuration

```python
llm_config:
  provider: "claude"
  model: "claude-sonnet-4-20250514"
  temperature: 0.1
  use_extended_thinking: true
  max_tokens: 4096
```

## ğŸ›¡ï¸ Security Features

- **Authentication**: JWT-based with refresh tokens
- **Authorization**: Role-based access control (RBAC)
- **Data Protection**: Encryption at rest and in transit
- **PII Handling**: Automatic detection and masking
- **Audit Logging**: Complete activity tracking
- **API Rate Limiting**: Configurable per endpoint

## ğŸ“ˆ Performance

- **30x Efficiency**: DocETL-inspired optimization
- **Caching**: Redis-based LLM response caching
- **Parallel Processing**: Distributed pipeline execution
- **Resource Management**: Automatic scaling and throttling

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guide](docs/CONTRIBUTING.md) for details.

### Development Workflow

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- DocETL framework for pipeline optimization concepts
- MDF Framework for multi-modal fusion strategies
- Claude Sonnet 4 for advanced AI capabilities

## ğŸ“ Support

- ğŸ“§ Email: support@datasetgen.ai
- ğŸ’¬ Discord: [Join our community](https://discord.gg/datasetgen)
- ğŸ“š Documentation: [docs.datasetgen.ai](https://docs.datasetgen.ai)
- ğŸ› Issues: [GitHub Issues](https://github.com/yourorg/dataset-gen/issues)